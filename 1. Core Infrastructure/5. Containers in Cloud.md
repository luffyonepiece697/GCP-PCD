# Introduction to Containers

* Infrastructure as a service, or IaaS, allows you to share compute resources with other developers by using virtual machines to virtualize the hardware.
* This lets each developer deploy their own operating system (OS), access the hardware, and build their applications in a self-contained environment 
  with access to RAM, file systems, networking interfaces, etc.
* This is where containers come in.
* The idea of a container is to give the independent scalability of workloads in PaaS and an abstraction layer of the OS and hardware in IaaS.
* A configurable system lets you install your favorite runtime, web server, database, or middleware, configure the underlying 
  system resources, such as disk space, disk I/O, or networking, and build as you like.
* But flexibility comes with a cost.
* The smallest unit of compute is an app with its VM.
* The guest OS might be large, even gigabytes in size, and take minutes to boot.
* As demand for your application increases, you have to copy an entire VM and boot the guest OS for each instance of your app, which can be slow and costly.
* A container is an invisible box around your code and its dependencies with limited access to its own partition of the file system and hardware.
* It only requires a few system calls to create and it starts as quickly as a process.
* All that’s needed on each host is an OS kernel that supports containers and a container runtime.
* In essence, the OS is being virtualized.
* It scales like PaaS but gives you nearly the same flexibility as IaaS.
* This makes code ultra portable, and the OS and hardware can be treated as a black box.
* So you can go from development, to staging, to production, or from your laptop to the cloud, without changing or rebuilding anything.
* As an example, let’s say you want to scale a web server.
* With a container, you can do this in seconds and deploy dozens or hundreds of them, depending on the size of your workload, on a single host.
* That's just a simple example of scaling one container running the whole application on a single host.
* However, you'll probably want to build your applications using lots of containers, each performing their own function like microservices.
* If you build them this way and connect them with network connections, you can make them modular, deploy easily, and scale independently across a group of hosts.
* The hosts can scale up and down and start and stop containers as demand for your app changes or as hosts fail.


# Kubernetes

* A product that helps manage and scale containerized applications is Kubernetes.
* So to save time and effort when scaling applications and workloads, Kubernetes can be bootstrapped using Google Kubernetes Engine (GKE).
* Kubernetes is an open-source platform for managing containerized workloads and services.
* It makes it easy to orchestrate many containers on many hosts, scale them as microservices, and easily deploy rollouts and rollbacks.
* At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster.
* The system is divided into a set of primary components that run as the control plane and a set of nodes that run containers.
* In Kubernetes, a node represents a computing instance, like a machine.
* Note that this is different to a node on Google Cloud which is a virtual machine running in Compute Engine.
* You can describe a set of applications and how they should interact with each other, and Kubernetes determines how to make that happen.

* Deploying containers on nodes by using a wrapper around one or more containers is what defines a Pod.
* A Pod is the smallest unit in Kubernetes that you can create or deploy.
* It represents a running process on your cluster as either a component of your application or an entire app.
* Generally, you only have one container per Pod, but if you have multiple containers with a hard dependency, 
  you can package them into a single Pod and share networking and storage resources between them.
* The Pod provides a unique network IP and set of ports for your containers and configurable options that govern how your containers should run.

* One way to run a container in a Pod in Kubernetes is to use the kubectl run command, which starts a Deployment with a container running inside a Pod.
* A Deployment represents a group of replicas of the same Pod and keeps your Pods running even when the nodes they run on fail.
* A Deployment could represent a component of an application or even an entire app.
* To see a list of the running Pods in your project, run the command: 
  <code>$ kubectl get pods</code>

* Kubernetes creates a Service with a fixed IP address for your Pods, and a controller says 
  "I need to attach an external load balancer with a public IP address to that Service so others outside the cluster can access it."
   <code>kubectl expose deployments nginx --port=80 --type=LoadBalancer</code>
* In GKE, the load balancer is created as a network load balancer.

* Any client that reaches that IP address will be routed to a Pod behind the Service.
<code>kubectl get service </code>
* A Service is an abstraction which defines a logical set of Pods and a policy by which to access them.
* As Deployments create and destroy Pods, Pods will be assigned their own IP addresses, but those addresses don't remain stable over time.
* A Service group is a set of Pods and provides a stable endpoint (or fixed IP address) for them.

* For example, if you create two sets of Pods called frontend and backend and put them behind their own Services, 
  the backend Pods might change, but frontend Pods are not aware of this.
* They simply refer to the backend Service.

* To scale a Deployment, run the kubectl scale command.
<code> kubectl scale </code>
* In this example, three Pods are created in your Deployment, and they're placed behind the Service and share one fixed IP address.
* You could also use autoscaling with other kinds of parameters.
* For example, you can specify that the number of Pods should increase when CPU utilization reaches a certain limit.

* So far, we’ve seen how to run imperative commands like expose and scale.
* This works well to learn and test Kubernetes step-by-step.
* But the real strength of Kubernetes comes when you work in a declarative way.
* Instead of issuing commands, you provide a configuration file that tells Kubernetes what you want your desired state to look like, 
  and Kubernetes determines how to do it.
* You accomplish this by using a Deployment config file.

* You can check your Deployment to make sure the proper number of replicas is running by using either kubectl get deployments or kubectl describe deployments.
<code>kubectl get deployments</code>
<code>kubectl decribe deployments</code>
* To run five replicas instead of three, all you do is update the Deployment config file and run the kubectl apply command to use the updated config file.
<code> kubectl apply -f nginx-deployment.yml </code>
* You can still reach your endpoint as before by using kubectl get services to get the external IP of the Service and reach the public IP address from a client.

* The last question is, what happens when you want to update a new version of your app?
* Well, you want to update your container to get new code in front of users, but rolling out all those changes at one time would be risky.
* So in this case, you would use kubectl rollout or change your deployment configuration file and then apply the change using kubectl apply.
<code> kubectl rollout </code>
* New Pods will then be created according to your new update strategy.


# Google Kubernetes Engine

* GKE is a Google-hosted managed Kubernetes service in the cloud.
* The GKE environment consists of multiple machines, specifically Compute Engine instances, grouped together to form a cluster.
* You can create a Kubernetes cluster with Kubernetes Engine, but how is GKE different from Kubernetes?
* From the user’s perspective, it’s a lot simpler.
* GKE manages all the control plane components for us.
* It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE takes responsibility for provisioning 
  and managing all the control plane infrastructure behind it.
* It also eliminates the need of a separate control plane.
* Node configuration and management depends on the type of GKE mode you use.
* With the Autopilot mode, which is recommended, GKE manages the underlying infrastructure such as node configuration, autoscaling, auto-upgrades, 
  baseline security configurations, and baseline networking configuration.
* With the Standard mode, you manage the underlying infrastructure, including configuring the individual nodes.

* Let’s examine the benefits and functionality of Autopilot in more detail.

* Autopilot is optimized for production.
* Autopilot also helps produce a strong security posture.
* And Autopilot also promotes operational efficiency.

* The GKE Standard mode has the same functionality as Autopilot, but you’re responsible for the configuration, management, and optimization of the cluster.
* Unless you require the specific level of configuration control offered by GKE standard, it’s recommended that you use Autopilot mode.

* You can create a Kubernetes cluster with Kubernetes Engine by using the Google Cloud console or the gcloud command that's provided by the 
  Cloud software development kit(Cloud SDK).
* GKE clusters can be customized, and they support different machine types, number of nodes, and network settings.

* Kubernetes provides the mechanisms through which you interact with your cluster.
* Kubernetes commands and resources are used to deploy and manage applications, perform administration tasks, set policies, and monitor the 
  health of deployed workloads.

* Running a GKE cluster comes with the benefit of advanced cluster management features that Google Cloud provides.
  1. Google Cloud's load-balancing for Compute Engine instances
  2. Node pools to designate subsets of nodes within a cluster for additional flexibility
  3. Automatic scaling of your cluster's node instance count
  4. Automatic upgrades for your cluster's node software
  5. Node auto-repair to maintain node health and availability
  6. And logging and monitoring with Google Cloud Observability for visibility into your cluster.
  
* To start up Kubernetes on a cluster in GKE, all you do is run this command: 
  <code>gcloud container clusters create k1</code>















